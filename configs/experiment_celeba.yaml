DATASET:
  PART_COUNT: 16
  SIZE: 202576
  #PATH: /_yucheng/dataSet/tfdatasets/tfrecords/celeba-r%02d.tfrecords.%03d
  PATH: /home/disanda/Desktop/dataSet/tf-celeba/celeba/tfrecords/celeba-r%02d.tfrecords.%03d
  MAX_RESOLUTION_LEVEL: 7 # 1 -> 4 -> 8 -> 16 -> 32 -> 64 -> 128 ->256 -> 512 -> 1024 （ 1->1, 2->4, 7-->128, 8->256, 9->512, 10->1024 ）
MODEL:
  # LATENT_SPACE_SIZE: 256
  LATENT_SPACE_SIZE: 512
  LAYER_COUNT: 6 # 就是有6个conv块
  MAX_CHANNEL_COUNT: 512
  START_CHANNEL_COUNT: 64 # 1024*1024: 64(128pixel) -> 32(256pixel) ->32(256pixel)  layer_count6 (128*128): channel 512 512 512 256 128 64
  DLATENT_AVG_BETA: 0.995
  # MAPPING_LAYERS: 5
  MAPPING_LAYERS: 8
OUTPUT_DIR: ./results/celeba512/
TRAIN:
  BASE_LEARNING_RATE: 0.0015 #这个用于adam内部
  EPOCHS_PER_LOD: [100,100,100,100,100,100] # 单个分辨率迭代的次数，每次的batch见 LOD_2_BATCH_1GPU
  LEARNING_DECAY_RATE: 0.1
  LEARNING_DECAY_STEPS: []
  TRAIN_EPOCHS: 600 #总的次数，会分摊到每个分辨率中
  LOD_2_BATCH_8GPU: [512, 256, 128,   64,   32,    32,     32,     32,     32,     32]
  LOD_2_BATCH_4GPU: [512, 256, 128,   64,   32,    16]
  LOD_2_BATCH_2GPU: [256, 256, 128,   64,   32,    16]
  LOD_2_BATCH_1GPU: [512, 256, 128,   64,   32,    16] 
  #LEARNING_RATES: [0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.002, 0.003, 0.003]
#每个分辨率的batch大小   4 ->8 ->16 ->  32 -> 64 -> 128->256->512
