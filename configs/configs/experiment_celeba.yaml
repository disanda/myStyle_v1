DATASET:
  PART_COUNT: 16
  SIZE: 202576
  PATH: /data/datasets/celeba/tfrecords/celeba-r%02d.tfrecords.%03d
  MAX_RESOLUTION_LEVEL: 7
MODEL:
  # LATENT_SPACE_SIZE: 256
  LATENT_SPACE_SIZE: 512
  LAYER_COUNT: 6
  MAX_CHANNEL_COUNT: 512
  START_CHANNEL_COUNT: 64
  DLATENT_AVG_BETA: 0.995
  # MAPPING_LAYERS: 5
  MAPPING_LAYERS: 8
OUTPUT_DIR: /data/celeba/results
TRAIN:
  BASE_LEARNING_RATE: 0.0015
  EPOCHS_PER_LOD: 6
  LEARNING_DECAY_RATE: 0.1
  LEARNING_DECAY_STEPS: []
  TRAIN_EPOCHS: 44
  LOD_2_BATCH_8GPU: [512, 256, 128,   64,   32,    32]
  LOD_2_BATCH_4GPU: [512, 256, 128,   64,   32,    16]
  LOD_2_BATCH_2GPU: [256, 256, 128,   64,   32,    16]
  LOD_2_BATCH_1GPU: [128, 128, 128,   64,   32,    16]
